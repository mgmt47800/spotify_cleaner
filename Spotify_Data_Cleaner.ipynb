{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spotify Data Cleaner\n",
        "\n",
        "This notebook cleans **personally identifiable information (PII)** from your downloaded Spotify account data so you can use it safely for class projects.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Place your downloaded Spotify data in a folder named **`Spotify Account Data`** in the same directory as this notebook (i.e. extract your Spotify export zip so that files like `Userdata.json`, `Identifiers.json`, and `Follow.json` are inside `Spotify Account Data/`).\n",
        "\n",
        "## What this notebook does\n",
        "\n",
        "1. **Replaces** identifiers (username, email, birthdate, names, etc.) with realistic fake values using the [Faker](https://faker.readthedocs.io/) library.\n",
        "2. **Writes** cleaned JSON files into a new folder, **`Spotify_Account_Data_Cleaned/`**, without modifying your original data.\n",
        "3. **Zips** that folder into a single file whose name is a **random number** (e.g. `71234987833478823.zip`) for anonymous submission.\n",
        "4. **Prints a summary** of every replacement made at the end.\n",
        "\n",
        "Run the cells in order from top to bottom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports and configuration\n",
        "import json\n",
        "import zipfile\n",
        "import shutil\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "from faker import Faker\n",
        "\n",
        "# Paths: read from Spotify Account Data, write to a new cleaned folder\n",
        "DATA_DIR = Path(\"Spotify Account Data\")\n",
        "OUTPUT_DIR = Path(\"Spotify_Account_Data_Cleaned\")\n",
        "ZIP_OUTPUT_DIR = Path(\".\")  # directory where the .zip file will be created (current folder)\n",
        "\n",
        "# Create the output folder (removes existing so each run starts clean)\n",
        "if OUTPUT_DIR.exists():\n",
        "    shutil.rmtree(OUTPUT_DIR)\n",
        "OUTPUT_DIR.mkdir(parents=True)\n",
        "\n",
        "# List to record every PII replacement for the final summary\n",
        "changes_summary = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Faker and consistent replacements\n",
        "\n",
        "We use **Faker** to generate fake but realistic values. To keep data consistent across files (e.g. the same email in `Userdata.json` and `Identifiers.json`), we use a single **mapping**: the first time we see an original value, we generate a fake replacement and store it; every later time we see that same value, we reuse the same replacement.\n",
        "\n",
        "You can change or remove the `seed` below to get different fake data on each run; using a seed makes results reproducible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Seeded Faker for reproducible replacements (change or remove seed for different results)\n",
        "fake = Faker(seed=42)\n",
        "pii_map = {}  # original value -> replacement value\n",
        "\n",
        "def get_replacement(original, generator):\n",
        "    \"\"\"Return a consistent fake replacement for `original`, recording in pii_map.\"\"\"\n",
        "    if original is None or (isinstance(original, str) and original.strip() == \"\"):\n",
        "        return original\n",
        "    if original not in pii_map:\n",
        "        pii_map[original] = generator()\n",
        "    return pii_map[original]\n",
        "\n",
        "def record_change(file_name, field, old_val, new_val):\n",
        "    \"\"\"Append a change to the summary list for the final report.\"\"\"\n",
        "    changes_summary.append({\n",
        "        \"file\": file_name,\n",
        "        \"field\": field,\n",
        "        \"old\": old_val,\n",
        "        \"new\": new_val,\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Clean `Userdata.json`\n",
        "\n",
        "`Userdata.json` contains account-level PII: **username**, **email**, **birthdate**, **gender**, **Facebook UID**, and **creation time**. We replace each with a Faker-generated value and write the result to the cleaned folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "userdata_path = DATA_DIR / \"Userdata.json\"\n",
        "with open(userdata_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    userdata = json.load(f)\n",
        "\n",
        "# Replace each PII field and record the change\n",
        "if userdata.get(\"username\"):\n",
        "    old_u = userdata[\"username\"]\n",
        "    userdata[\"username\"] = get_replacement(old_u, fake.user_name)\n",
        "    record_change(\"Userdata.json\", \"username\", old_u, userdata[\"username\"])\n",
        "\n",
        "if userdata.get(\"email\"):\n",
        "    old_e = userdata[\"email\"]\n",
        "    userdata[\"email\"] = get_replacement(old_e, fake.email)\n",
        "    record_change(\"Userdata.json\", \"email\", old_e, userdata[\"email\"])\n",
        "\n",
        "if userdata.get(\"birthdate\"):\n",
        "    old_b = userdata[\"birthdate\"]\n",
        "    userdata[\"birthdate\"] = get_replacement(old_b, lambda: fake.date_of_birth(minimum_age=18, maximum_age=80).strftime(\"%Y-%m-%d\"))\n",
        "    record_change(\"Userdata.json\", \"birthdate\", old_b, userdata[\"birthdate\"])\n",
        "\n",
        "if userdata.get(\"gender\"):\n",
        "    old_g = userdata[\"gender\"]\n",
        "    userdata[\"gender\"] = get_replacement(old_g, lambda: random.choice([\"male\", \"female\", \"non-binary\"]))\n",
        "    record_change(\"Userdata.json\", \"gender\", old_g, userdata[\"gender\"])\n",
        "\n",
        "if userdata.get(\"facebookUid\"):\n",
        "    old_f = userdata[\"facebookUid\"]\n",
        "    userdata[\"facebookUid\"] = get_replacement(old_f, lambda: fake.numerify(text=\"###############\"))\n",
        "    record_change(\"Userdata.json\", \"facebookUid\", old_f, userdata[\"facebookUid\"])\n",
        "\n",
        "if userdata.get(\"creationTime\"):\n",
        "    old_c = userdata[\"creationTime\"]\n",
        "    userdata[\"creationTime\"] = get_replacement(old_c, lambda: fake.date_between(start_date=\"-15y\", end_date=\"-1y\").strftime(\"%Y-%m-%d\"))\n",
        "    record_change(\"Userdata.json\", \"creationTime\", old_c, userdata[\"creationTime\"])\n",
        "\n",
        "with open(OUTPUT_DIR / \"Userdata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(userdata, f, indent=2)\n",
        "\n",
        "print(\"Userdata.json cleaned and written to\", OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clean `Identifiers.json`\n",
        "\n",
        "This file stores login identifiers (e.g. email). We replace `identifierValue` with the **same** fake email used in `Userdata.json` for that original email, so the account stays consistent across files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "identifiers_path = DATA_DIR / \"Identifiers.json\"\n",
        "with open(identifiers_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    identifiers = json.load(f)\n",
        "\n",
        "# Replace email with same mapping as Userdata (so same original -> same fake)\n",
        "if identifiers.get(\"identifierType\") == \"email\" and identifiers.get(\"identifierValue\"):\n",
        "    old_val = identifiers[\"identifierValue\"]\n",
        "    identifiers[\"identifierValue\"] = get_replacement(old_val, fake.email)\n",
        "    record_change(\"Identifiers.json\", \"identifierValue\", old_val, identifiers[\"identifierValue\"])\n",
        "\n",
        "with open(OUTPUT_DIR / \"Identifiers.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(identifiers, f, indent=2)\n",
        "\n",
        "print(\"Identifiers.json cleaned and written to\", OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Clean `Follow.json`\n",
        "\n",
        "`Follow.json` contains display names in **userIsFollowing**, **userIsFollowedBy**, and **userIsBlocking**. We replace each name with a consistent fake name (e.g. first name + last initial)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def replace_follow_names(data, list_key, file_name):\n",
        "    \"\"\"Replace each display name in the given list with a fake name; record changes.\"\"\"\n",
        "    if list_key not in data or not isinstance(data[list_key], list):\n",
        "        return\n",
        "    new_list = []\n",
        "    for name in data[list_key]:\n",
        "        if name:\n",
        "            new_name = get_replacement(name, lambda: f\"{fake.first_name()} {fake.last_name()[0]}.\")\n",
        "            record_change(file_name, list_key, name, new_name)\n",
        "            new_list.append(new_name)\n",
        "        else:\n",
        "            new_list.append(name)\n",
        "    data[list_key] = new_list\n",
        "\n",
        "follow_path = DATA_DIR / \"Follow.json\"\n",
        "with open(follow_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    follow = json.load(f)\n",
        "\n",
        "replace_follow_names(follow, \"userIsFollowing\", \"Follow.json\")\n",
        "replace_follow_names(follow, \"userIsFollowedBy\", \"Follow.json\")\n",
        "replace_follow_names(follow, \"userIsBlocking\", \"Follow.json\")\n",
        "\n",
        "with open(OUTPUT_DIR / \"Follow.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(follow, f, indent=2)\n",
        "\n",
        "print(\"Follow.json cleaned and written to\", OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Clean `Inferences.json`\n",
        "\n",
        "`Inferences.json` contains demographic and behavioral segments (e.g. `demographic_age_50-54`, `demographic_male`). We replace age bands and gender with generic values so the data remains useful for analysis but not identifying."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generic replacements for demographic segments (consistent per original value)\n",
        "GENERIC_AGE_BANDS = [\"demographic_age_25-29\", \"demographic_age_30-34\", \"demographic_age_35-40\"]\n",
        "GENERIC_GENDERS = [\"demographic_male\", \"demographic_female\"]\n",
        "\n",
        "def replace_inference(original):\n",
        "    \"\"\"Replace a single inference string if it is demographic age or gender.\"\"\"\n",
        "    if re.match(r\"demographic_age_\\d+-\\d+\", original):\n",
        "        return get_replacement(original, lambda: random.choice(GENERIC_AGE_BANDS))\n",
        "    if original in (\"demographic_male\", \"demographic_female\"):\n",
        "        return get_replacement(original, lambda: random.choice(GENERIC_GENDERS))\n",
        "    return original\n",
        "\n",
        "inferences_path = DATA_DIR / \"Inferences.json\"\n",
        "with open(inferences_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    inferences_data = json.load(f)\n",
        "\n",
        "new_inferences = []\n",
        "for val in inferences_data.get(\"inferences\", []):\n",
        "    new_val = replace_inference(val)\n",
        "    if new_val != val:\n",
        "        record_change(\"Inferences.json\", \"inferences\", val, new_val)\n",
        "    new_inferences.append(new_val)\n",
        "inferences_data[\"inferences\"] = new_inferences\n",
        "\n",
        "with open(OUTPUT_DIR / \"Inferences.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(inferences_data, f, indent=2)\n",
        "\n",
        "print(\"Inferences.json cleaned and written to\", OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Clean playlist names in `Playlist1.json`\n",
        "\n",
        "Playlist **name** fields can contain first names (e.g. \"Rob and Harper Christmas Mix\", \"Harper's Favorites\"). We detect common first names in each name string and replace them with Faker first names, reusing the same mapping so the same name is always replaced the same way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build a set of common first names (including names that often appear in playlist titles)\n",
        "# Faker's en_US provider; we add a few more that are common in playlist names\n",
        "COMMON_FIRST_NAMES = {\n",
        "    \"Harper\", \"Rob\", \"Jake\", \"Alex\", \"Sam\", \"Jordan\", \"Taylor\", \"Morgan\", \"Casey\",\n",
        "    \"Jamie\", \"Quinn\", \"Avery\", \"Riley\", \"Hayden\", \"Parker\", \"Drew\", \"Blake\", \"Reese\",\n",
        "    \"James\", \"John\", \"Robert\", \"Michael\", \"David\", \"William\", \"Richard\", \"Joseph\",\n",
        "    \"Mary\", \"Patricia\", \"Jennifer\", \"Linda\", \"Elizabeth\", \"Barbara\", \"Susan\", \"Jessica\",\n",
        "    \"Chris\", \"Nick\", \"Matt\", \"Mike\", \"Dave\", \"Tom\", \"Steve\", \"Joe\", \"Dan\", \"Ben\",\n",
        "    \"Amy\", \"Kate\", \"Sarah\", \"Emily\", \"Anna\", \"Laura\", \"Lisa\", \"Rachel\", \"Megan\",\n",
        "}\n",
        "\n",
        "def replace_first_names_in_string(text):\n",
        "    \"\"\"Replace any common first name in text with a Faker first name; return new string and record changes.\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return text\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        # Handle possessive: \"Harper's\" -> base \"Harper\", suffix \"'s\"\n",
        "        base = re.sub(r\"'s$\", \"\", word, flags=re.IGNORECASE)\n",
        "        if base in COMMON_FIRST_NAMES:\n",
        "            replacement = get_replacement(base, fake.first_name)\n",
        "            record_change(\"Playlist1.json\", \"playlist name\", base, replacement)\n",
        "            # Preserve possessive if original had it\n",
        "            if word.endswith(\"'s\") or word.endswith(\"'S\"):\n",
        "                new_words.append(replacement + \"'s\")\n",
        "            else:\n",
        "                new_words.append(replacement)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "playlist_path = DATA_DIR / \"Playlist1.json\"\n",
        "with open(playlist_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    playlists_data = json.load(f)\n",
        "\n",
        "for playlist in playlists_data.get(\"playlists\", []):\n",
        "    if playlist.get(\"name\"):\n",
        "        old_name = playlist[\"name\"]\n",
        "        playlist[\"name\"] = replace_first_names_in_string(old_name)\n",
        "\n",
        "with open(OUTPUT_DIR / \"Playlist1.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(playlists_data, f, indent=2)\n",
        "\n",
        "print(\"Playlist1.json cleaned and written to\", OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Copy all remaining files unchanged\n",
        "\n",
        "All other files (e.g. **StreamingHistory**, **YourLibrary**, **Marquee**, **Payments**, other playlists) are copied into the cleaned folder so the dataset stays complete and the output has the same structure as the original. Only the five files we modified above are written with obfuscated data; everything else is copied as-is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Which files we already wrote (cleaned) â€” only these are skipped when copying\n",
        "cleaned_files = {\"Userdata.json\", \"Identifiers.json\", \"Follow.json\", \"Inferences.json\", \"Playlist1.json\"}\n",
        "\n",
        "# Copy every file from the source folder (including subdirs) into OUTPUT_DIR, preserving structure.\n",
        "# Skip only the five files we already wrote with obfuscated data.\n",
        "for path in DATA_DIR.rglob(\"*\"):\n",
        "    if not path.is_file():\n",
        "        continue\n",
        "    rel = path.relative_to(DATA_DIR)\n",
        "    # Skip top-level files we already cleaned and wrote\n",
        "    if len(rel.parts) == 1 and rel.name in cleaned_files:\n",
        "        continue\n",
        "    dest = OUTPUT_DIR / rel\n",
        "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy2(path, dest)\n",
        "    print(\"Copied:\", rel)\n",
        "\n",
        "print(\"All remaining files copied to\", OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Create ZIP with random numeric filename\n",
        "\n",
        "The cleaned folder is zipped so you can submit or share it easily. The filename is a **random number** (e.g. `71234987833478823.zip`) so it does not identify you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Random numeric filename (e.g. 71234987833478823.zip); same number used as folder name inside zip\n",
        "zip_stem = str(random.randint(10**16, 10**18 - 1))\n",
        "zip_name = zip_stem + \".zip\"\n",
        "zip_path = ZIP_OUTPUT_DIR / zip_name\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "    for f in OUTPUT_DIR.rglob(\"*\"):\n",
        "        if f.is_file():\n",
        "            arcname = f\"{zip_stem}/{f.relative_to(OUTPUT_DIR)}\"  # e.g. 71234987833478823/Userdata.json\n",
        "            zf.write(f, arcname)\n",
        "\n",
        "print(\"Created zip:\", zip_path.resolve())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of changes\n",
        "\n",
        "Below is a report of every PII replacement made in this run. Use it to verify what was anonymized before you submit or share the zip file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Group changes by file for a clear report\n",
        "from collections import defaultdict\n",
        "by_file = defaultdict(list)\n",
        "for c in changes_summary:\n",
        "    by_file[c[\"file\"]].append((c[\"field\"], c[\"old\"], c[\"new\"]))\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SUMMARY OF PII REPLACEMENTS\")\n",
        "print(\"=\" * 60)\n",
        "for file_name in sorted(by_file.keys()):\n",
        "    print(f\"\\n--- {file_name} ---\")\n",
        "    for field, old_val, new_val in by_file[file_name]:\n",
        "        print(f\"  {field}:  {old_val!r}  ->  {new_val!r}\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Total replacements: {len(changes_summary)}\")\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}